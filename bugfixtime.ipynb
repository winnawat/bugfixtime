{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bugfixtime\n",
    "Predicting how many days it would take to fix a bug, given the Jira information when the bug is filed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Bugs and software develeopment are inseparable. Also, having bugs in software products implies that at least some man-hour need to be assigned to address them. According to [a study in 2002 by America's National Institute of Standards and Technology (NIST)](http://www.abeacha.com/NIST_press_release_bugs_cost.htm), software bugs cost the U.S. economy an estimated **$59.5 billion** annually, or about 0.6 percent of the gross domestic product. It is almost impossible to expect no bug at all, hence, the second best alternative appears to be knowing how much work is needed to fix them so resources can be maanged efficiently.  \n",
    "This project attempts to chip away at that problem by trying to predict how many days it would take to fix a bug, given their metadata when they are filed in Jira."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The motivation above helps to frame the problem statement into the following,  \n",
    "*Given the metadata of the bug filed on Jira, predict how many days it would take to close/fix it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The data used in this study is obtained from the research article titled [From Reports to Bug-Fix Commits: A 10 Years Dataset of Bug-Fixing Activity from 55 Apache's Open Source Projects](https://dl.acm.org/doi/10.1145/3345629.3345639) by Vieira, Da Silva, Rocha, and Gomes in 2019. The data is housed [here](https://figshare.com/articles/Replication_Package_-_PROMISE_19/8852084).  \n",
    "Contained within is a dataset composed of more than 70,000 bug-fix reports from 10 years of bug-fixing activity of 55 projects from the Apache Software Foundation along with their Jira data and status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Running `mining-script.py` as specified by the README file in the research article generates 3 CSV files for each of the 55 Apache Software project. The data used in here is from the `<projectname>-full-bug-fix-dataset` file from each project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn import preprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the file with project names\n",
    "The CSV files generated by `mining-script.py` come in the format `<projectname>-jira-bug-fix-dataset`. Hence, the list of project names is needed to read the CSV files with different project name as prefixes. This list of project name can be found in the file `projects.csv`. However, the project names in the file `projects.csv` does not match the file names exactly and some names have to be added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read projects.csv\n",
    "projects = pd.read_csv(\"data/projects.csv\", delimiter=\";\")\n",
    "\n",
    "# format the pandas series into list\n",
    "projectsnames = list(projects[\"Name\"].sort_values())\n",
    "\n",
    "# missing names\n",
    "missingnames = [\"mng\",\n",
    "                \"mrm\",\n",
    "                \"dirkrb\",\n",
    "                \"dirmina\",\n",
    "                \"fc\",\n",
    "                \"flink\",\n",
    "                \"oozie\",\n",
    "                \"hadoop\",\n",
    "                \"hbase\",\n",
    "                \"hdfs\",\n",
    "                \"mapreduce\",\n",
    "                \"tap5\",\n",
    "                \"ww\",\n",
    "                \"yarn\"]\n",
    "\n",
    "# add missing project names\n",
    "projectsnames = projectsnames + missingnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above is used to create CSV paths for each project's data. Reading the CSV from paths in the list then gives the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files read: 0 (expect 56)\n"
     ]
    }
   ],
   "source": [
    "# create a list of CSV paths for the for loop below\n",
    "csvpaths = []\n",
    "stopwords = [\"commons\", \"core\", \"mina\"]\n",
    "for name in projectsnames:\n",
    "    name = name.lower()\n",
    "    namesplit = name.split(\" \")\n",
    "    namesplit  = [word for word in namesplit if word not in stopwords]\n",
    "    name = \"\".join(namesplit)\n",
    "    path = \"./bug-fix/\" + name + \"-jira-bug-fix-dataset.csv\"\n",
    "    csvpaths.append(path)\n",
    "\n",
    "# for loop to read CSV, append to a dataframe and count the number of files read\n",
    "filesread = 0\n",
    "bugsdf = pd.DataFrame()\n",
    "for path in csvpaths:\n",
    "    try:\n",
    "        somecsv = pd.read_csv(path, delimiter=\";\")\n",
    "        filesread += 1\n",
    "        bugsdf = bugsdf.append(somecsv, ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "print(\"files read: %i (expect 56)\" % filesread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "This section involves examining key aspects of the data such as column names and types, reformatting and cleaning them as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names and type\n",
    "Looking at the column names and their datatype can give a rough idea of what can be used as features for the prediction models. The details about what the columns mean can be found in the PDF accompanying the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
